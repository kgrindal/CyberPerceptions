---
title: "Cyber_Perceptions_Survey_Proj"
author: "Karl Grindal"
date: "11/13/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# setwd<-("C:/Users/karl_000/Documents/SpiderOak Hive/GeorgiaTech")
library(dplyr)
library(qualtRics)
library(corrplot)
library(here)
library(ppcor)
library(psych)
library(tidyverse)
library(gridExtra)
library(nnet)

raw_data <- read_survey("C:/Users/karl_000/Documents/SpiderOak Hive/GeorgiaTech/Fall_2020/Cyber_Perceptions/Cyber_Perceptions_Survey_v9.csv")

```

# Clean Data for Analysis
```{r}

# Clean Column Names
names(raw_data) <- gsub("\\.", "", names(raw_data))

# Drop Columns
raw_data <- raw_data[ , -which(names(raw_data) %in% c("Status","Progress","Finished","RecordedDate","RecipientLastName","RecipientFirstName",
                                                      "RecipientEmail","ExternalReference","UserLanguage","Q1","role","organization"))]

# Remove Incomplete Data
raw_data <- raw_data[complete.cases(raw_data$LocationLatitude),]

# Remove Duplicative Responses
raw_data <- raw_data[!duplicated(raw_data$IPAddress), ]

# Breakout Data
Userdata <- subset(raw_data[,1:which(colnames(raw_data)=="DistributionChannel")])
Questions <- subset(raw_data[,which(colnames(raw_data)=="1Q1"):which(colnames(raw_data)=="5Q4")])
Controls <- subset(raw_data[,which(colnames(raw_data)=="Q11"):which(colnames(raw_data)=="Q9")])
Scenario <- subset(raw_data[,which(colnames(raw_data)=="Time Lag 1" ):NCOL(raw_data)])
```

#Fix Userdata
```{r}
NewUserData <- rbind(Userdata,Userdata,Userdata,Userdata,Userdata)

```

#Fix Questions
```{r}
Questions1 <- Questions[,c("1Q1","1Q2","1Q3","1Q4")]
Questions2 <- Questions[,c("2Q1","2Q2","2Q3","2Q4")]
Questions3 <- Questions[,c("3Q1","3Q2","3Q3","3Q4")]
Questions4 <- Questions[,c("4Q1","4Q2","4Q3","4Q4")]
Questions5 <- Questions[,c("5Q1","5Q2","5Q3","5Q4")]

Questions1$ID <- 1
Questions2$ID <- 2
Questions3$ID <- 3
Questions4$ID <- 4
Questions5$ID <- 5

dfs <- c("Questions1", "Questions2", "Questions3", "Questions4", "Questions5")
for(df in dfs)
  assign(df, setNames(get(df),  c("AttackDef","AssessConf","Response","Norm", "ID")))

NewQuestions <- rbind(Questions1,Questions2,Questions3,Questions4,Questions5)

cols <- c("AttackDef","AssessConf","Response","Norm", "ID")
NewQuestions[cols] <- lapply(NewQuestions[cols], factor)  


```


# Fix Controls
```{r}
Controls$KQ1 <- if_else((Controls$Q11 ==TRUE),0,1)
Controls$KQ2 <- if_else((Controls$Q12 ==TRUE),0,1)
Controls$KQ3 <- if_else((Controls$Q14 ==TRUE),1,0)
Controls$KQ4 <- if_else((Controls$Q24 ==TRUE),0,1)
Controls$KQ5 <- if_else((Controls$Q13 ==TRUE),1,0)
Controls$KQ6 <- if_else((Controls$Q25 =="Secure"),1,0)
Controls$KQ7 <- if_else((Controls$Q26 =="True"),1,0)

Controls$KSUM <- (rowSums(Controls[,c("KQ1","KQ2","KQ3","KQ4","KQ5","KQ6","KQ7")])/7)

Controls$R1 <- Controls$Q29_1
Controls$R1 <- (Controls$R1 / 7)

prob <- c("Definitely take my winnings"=1,"Probably take my winnings"=2,"Not sure"=3,"Probably continue playing"=4,"Definitely continue playing"=5)
Controls$R2 <- prob[Controls$Q30]

likert <- c("Strongly agree"=5,"Somewhat agree"=4,"Neither agree nor disagree"=3,"Somewhat disagree"=2,"Strongly disagree"=1)
Controls$R3 <- likert[Controls$Q31]
Controls$R4 <- likert[Controls$Q32]
Controls$R5 <- likert[Controls$Q34]
Controls$R6 <- likert[Controls$Q35]
easy <- c("Extremely difficult"=1,"Somewhat difficult"=2,"Neither easy nor difficult"=3,"Somewhat easy"=4,"Extremely easy"=5)
Controls$R7 <- easy[Controls$Q36]

Controls$RSUM <- (rowSums(Controls[,c("R2","R3","R4","R5","R6","R7")])/30)
Controls$RSUMED <- (rowSums(Controls[,c("R1","RSUM")])/2)

Controls$M1 <- if_else((Controls$Q17 =="Yes"),1,0)
Controls$M2 <- if_else((Controls$Q19 =="Yes"),1,0)
Controls$M3 <- if_else((Controls$Q20 =="Yes"),1,0)

Controls$MSUM <- (rowSums(Controls[,c("M1","M2","M3")])/3)

CleanControls <- subset(Controls[,c("Q6","Q7","Q8","KSUM","RSUMED","MSUM")])

American_list <- c("American (American)","American (Caucasian)","American citizen","USA (Caucasian)","American (USA)","US Citizen","USA","US","us","American","United States",
                   "american","U.S.A.","United states","usa","U.S.")
cut_America<- paste0("\\b(", paste0(American_list, collapse="|"), ")\\b")

CleanControls$Q8 <- gsub(cut_America, "American", CleanControls$Q8)

cols <- c("Q6","Q7","Q8")
CleanControls[cols] <- lapply(CleanControls[cols], factor)  

summary(CleanControls)

NewControls <- rbind(CleanControls,CleanControls,CleanControls,CleanControls,CleanControls)



```


# Fix Scores
```{r}
# Score Stacking
Scenario1 <- Scenario[,c("Time Lag 1","Attribution Confidence 1","Damage Assessment 1","Hack type 1","Persistence 1")]
Scenario2 <- Scenario[,c("Time Lag 2","Attribution Confidence 2","Damage Assessment 2","Hack type 2","Persistence 2")]
Scenario3 <- Scenario[,c("Time Lag 3","Attribution Confidence 3","Damage Assessment 3","Hack type 3","Persistence 3")]
Scenario4 <- Scenario[,c("Time Lag 4","Attribution Confidence 4","Damage Assessment 4","Hack type 4","Persistence 4")]
Scenario5 <- Scenario[,c("Time Lag 5","Attribution Confidence 5","Damage Assessment 5","Hack type 5","Persistence 5")]

# Set IDs
Scenario1$ID <- 1
Scenario2$ID <- 2
Scenario3$ID <- 3
Scenario4$ID <- 4
Scenario5$ID <- 5

dfs <- c("Scenario1", "Scenario2", "Scenario3", "Scenario4", "Scenario5")
for(df in dfs)
  assign(df, setNames(get(df),  c("Time_Lag","Attribution_Confidence","Damage_Assessment","Hack_Type","Persistence", "ID")))

Scenarios <- rbind(Scenario1,Scenario2,Scenario3,Scenario4,Scenario5)

cols <- c("Time_Lag","Attribution_Confidence","Damage_Assessment","Hack_Type","Persistence", "ID")
Scenarios[cols] <- lapply(Scenarios[cols], factor)  

summary(Scenarios)

#Combine Components
stacked_data <- cbind(NewUserData,NewControls,Scenarios,NewQuestions)

levels(stacked_data$Response)[match("Escalate",levels(stacked_data$Response))] <- "Escalatory attack" # Fixed a survey answer response error affecting 1 question for 4 respondents.


table(stacked_data$AttackDef, stacked_data$Response)
table(stacked_data$AttackDef, stacked_data$Response,  stacked_data$Norm)

stacked_data

```





# Date Reducation to Creat Cross Correlation Matrix
```{r}
#Removing ID variable
View(stacked_data)

data1 <- subset(stacked_data, select = c("communicativity","Response","Norm","Damage_Assessment","Hack_Type","Attribution_Confidence","Persistence","Time_Lag"))

data1$Damage_Assessment <- as.numeric(data1$Damage_Assessment)
data1$Hack_Type <- as.numeric(data1$Hack_Type)
data1$Attribution_Confidence <- as.numeric(data1$Attribution_Confidence)
data1$Persistence <- as.numeric(data1$Persistence)
data1$Time_Lag <- as.numeric(data1$Time_Lag)
data1$Response <- as.numeric(data1$Response)
data1$Norm <- as.numeric(data1$Norm)

datamatrix<-cor(data1)
corrplot(datamatrix, method="number")

# Fix this...

# high correlations are Hack_Type:Attribution_Confidence & Attribution_Confidence:Persistence & Time_Lag:Persistence
# data2 <- data1[,-c("Norm")]
# pcor(data2, method="pearson")

# Level2: Attribution_Confidence*Persistence + Time_Lag*Persistence
# KMO(r=datamatrix)
# print(cortest.bartlett(datamatrix,nrow(data1)))
# parallel <- fa.parallel(data1, fm = "minres", fa = "fa")

```

# Creating a communicativity variable
```{r}
summary(stacked_data$communicativity)
table(stacked_data$communicativity)


stacked_data$AssessConf <- as.character(stacked_data$AssessConf)
stacked_data

stacked_data$AttackDefNeg <- if_else((stacked_data$AttackDef =="Sylvania is signaling their opposition to our prior high intensity attack. They want to strategically deter attacks of this kind in the future."),.5,-.5)

class(stacked_data$'AttackDefNo')
class(stacked_data$'ResponseId')


likert2 <- c("extremely unconfident"=1,"not very confident"=2,"somewhat confident"=3,"very confident"=4,"extremely confident"=5)
stacked_data$AssessConfNo <- likert2[stacked_data$AssessConf]

stacked_data$AttackDefNo <- if_else((stacked_data$AttackDef =="Sylvania is signaling their opposition to our prior high intensity attack. They want to strategically deter attacks of this kind in the future."),2,1)
stacked_data$AttackDefNo <- as.numeric(stacked_data$AttackDefNo)

stacked_data$communicativity <- if_else(stacked_data$AttackDefNo == 2,abs(stacked_data$AssessConfNo-6),stacked_data$AssessConfNo+5)

stacked_data$communicativity <- as.numeric(stacked_data$communicativity)


```

# Plotting Outliers
```{r}

# Plot of data with outliers.

plot1 <- ggplot(data = data1, aes(x = Attribution_Confidence, y = communicativity)) +
        geom_point() + 
        geom_smooth(method = lm) +
        xlim(0, 3) + ylim(0, 10) + 
        ggtitle("No Outliers")
plot2 <- ggplot(data = data1, aes(x = Attribution_Confidence, y = communicativity)) +
        geom_point() + 
        geom_smooth(method = lm) +
        xlim(0, 3) + ylim(0, 10) + 
        ggtitle("With Outliers")

gridExtra::grid.arrange(plot1, plot2, ncol=2)


mod <- lm(communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence, data=stacked_data)
cooksd <- cooks.distance(mod)

sample_size <- nrow(stacked_data)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance

with(summary(logitregH1c), 1 - deviance/null.deviance) # maybe is this the R squared?
par(mfrow = c(2, 2))
plot(logitregH1c)

# survey_new <- stacked_data[-c(), ]

# foo <- function(a,b) c(quotient = floor(a / b), modulo = a %% b)
# test <-c(a,b,c)

# Test for outliers based on the student
# foo(test,72)

```

# Hypothesis 1: Without interaction effects
```{r}

# Hypothesis 

logitregH1 <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence, data = stacked_data)
summary(logitregH1)

#Type a message

logitregH1b <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence, data = stacked_data)
summary(logitregH1b)

logitregH1c <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence, data = stacked_data)
summary(logitregH1c)

logitregH1d <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + KSUM + RSUMED + MSUM, data = stacked_data)
summary(logitregH1d)

```

# Hypothesis 2: With interaction effects
```{r}

# Hypothesis includes all possible interactions (coercive signal)
logitregH2a <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * 
                     Time_Lag, data = data1)
summary(logitregH2a)
summary(logitregH2a)$coeff[-1,4]<0.05

# Only includes interactions of the Message variables with other message variables and Context with other context variables
logitregH2b <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Damage_Assessment * Hack_Type + Damage_Assessment * 
                     Attribution_Confidence + Attribution_Confidence * Hack_Type
                   + Persistence + Time_Lag + Persistence * Time_Lag, data = data1)
summary(logitregH2b)

# Running a regression with all interaction effects betweent two variables (excludes interaction effects between three)
logitregH2c <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + Damage_Assessment * Hack_Type 
                   + Damage_Assessment * Attribution_Confidence + Attribution_Confidence * Hack_Type  + Persistence * Time_Lag + Damage_Assessment * Persistence
                   + Hack_Type * Persistence + Attribution_Confidence * Persistence + Damage_Assessment * Time_Lag + Hack_Type * Time_Lag + Attribution_Confidence * 
                     Time_Lag, data = data1)
summary(logitregH2c)
## high correlations are Hack_Type:Attribution_Confidence & Attribution_Confidence:Persistence & Time_Lag:Persistence

# Regression to evaluate whether interaction effects that showed high correlation in a cor table significantly alter the model
logitregH2d <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + Hack_Type*Time_Lag, data = stacked_data) # Hack_Type*Time_Lag did not show in cor table, but is significant

summary(logitregH2d) # no effect or significane
summary(logitregH2d)$coeff[-1,4]<0.05

```


# Hypothesis 3 on Escalation
```{r}

# this subsetted data to only communicative attacks

dataH3 <- subset(data1, communicativity>5)

# run with multinom
logitregH3a <- multinom(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=dataH3)
summary(logitregH3a) # nothing is statistically significant

# run with ordinal
logitregH3a2 <- polr(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=stacked_data)
summary(logitregH3a2) # nothing is statistically significant


logitregH3b <- multinom(Response ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * Time_Lag, data=dataH3)
summary(logitregH3b) # nothing is statistically significant

#running same model with ordinal
logitregH3b2 <- polr(Response ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * Time_Lag, data=stacked_data)
summary(logitregH3b2) # nothing is statistically significant

# Removing the constraint of perceiving communicative attacks

logitregH3c <- multinom(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=data1)
summary(logitregH3c) # nothing is statistically significant

# nothing is statistically significant
logitregH3d <- multinom(Response ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * Time_Lag, data=data1)
summary(logitregH3d) # nothing is statistically significant


```


# Hypothesis 4 on Norm Adoption --- Future research could focus on!
```{r}

dataH4 <- na.omit(dataH3)

nrow(dataH4) # 31 results not sufficient for statistical tests
summary(dataH4)

logitregH4a <- glm(Norm ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=dataH4)
summary(logitregH4a)

# assumptions:
# 1) identify if attack is communicative
# 2) assign confidence level to communicative assessment
# 3) choose response (escalate, proportional, deescalate) for short term response
# 4) in the long-term either abide or reject the proposed communicated norm

```

# Hypothesis 5 on Effect of Supplementary Variables on Confidence
```{r}

logitregH5a <- glm(formula = AssessConfNo ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + KSUM + RSUMED + MSUM, data = stacked_data)
summary(logitregH1d)

```
# Hypothesis 6 Employing a logit on instrumentality vs communicativity

```{r}

stacked_data$AttackDefNo <- (stacked_data$AttackDefNo - 1)
logitregH6a <- glm(formula = AttackDefNo ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + KSUM + RSUMED + MSUM, family = "binomial", 
                   data = stacked_data) 
summary(logitregH6a)
```






