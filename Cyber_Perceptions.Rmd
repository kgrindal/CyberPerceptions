---
title: "Cyber_Perceptions_Survey_Proj"
author: "Karl Grindal"
date: "03/15/2021"
output:
  pdf_document: default
  html_document: default
---

# Load Libraries and Create Anon File from Raw Qualtrics Output
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(qualtRics)
library(corrplot)
library(here)
library(ggplot2)
library(nnet)
library(MASS)

raw_data <- read_survey("C:/Users/karl_000/Documents/SpiderOak Hive/GeorgiaTech/Fall_2020/Cyber_Perceptions/Cyber_Perceptions_Survey_v9.csv")

# Remove Duplicative Responses
raw_data <- raw_data[!duplicated(raw_data$IPAddress), ]

anon_data <- raw_data[ , -which(names(raw_data) %in% 
                                 c("StartDate","EndDate","Status","IPAddress","RecordedDate","RecipientLastName","RecipientFirstName",
                                   "RecipientEmail","ExternalReference","LocationLatitude","LocationLongitude","role","organization"))]

write.csv(anon_data,"anon_data.csv")

```

# Clean Data for Analysis
```{r}
anon_data <- read.csv(here::here("anon_data.csv"), sep=",")
  
# Clean Column Names
names(anon_data) <- gsub("\\.", "", names(anon_data))

# Remove Incomplete Data
anon_data <- filter(anon_data, Progress == 100)
# names(anon_data)

# Breakout Data
Userdata <- subset(anon_data[,1:which(colnames(anon_data)=="UserLanguage")])
Questions <- subset(anon_data[,which(colnames(anon_data)=="Q1"):which(colnames(anon_data)=="X5Q4")])
Controls <- subset(anon_data[,which(colnames(anon_data)=="Q11"):which(colnames(anon_data)=="Q9")])
Scenario <- subset(anon_data[,which(colnames(anon_data)=="TimeLag1" ):NCOL(anon_data)])
```

# Fix Userdata
```{r}
NewUserData <- rbind(Userdata,Userdata,Userdata,Userdata,Userdata)

```

# Fix Questions
```{r}

Questions1 <- Questions[,c("X1Q1","X1Q2","X1Q3","X1Q4")]
Questions2 <- Questions[,c("X2Q1","X2Q2","X2Q3","X2Q4")]
Questions3 <- Questions[,c("X3Q1","X3Q2","X3Q3","X3Q4")]
Questions4 <- Questions[,c("X4Q1","X4Q2","X4Q3","X4Q4")]
Questions5 <- Questions[,c("X5Q1","X5Q2","X5Q3","X5Q4")]

Questions1$ID <- 1
Questions2$ID <- 2
Questions3$ID <- 3
Questions4$ID <- 4
Questions5$ID <- 5

dfs <- c("Questions1", "Questions2", "Questions3", "Questions4", "Questions5")
for(df in dfs)
  assign(df, setNames(get(df),  c("AttackDef","AssessConf","Response","Norm", "ID")))

NewQuestions <- rbind(Questions1,Questions2,Questions3,Questions4,Questions5)

cols <- c("AttackDef","AssessConf","Response","Norm", "ID")

```


# Fix Controls
```{r}
Controls$KQ1 <- if_else((Controls$Q11 ==TRUE),0,1)
Controls$KQ2 <- if_else((Controls$Q12 ==TRUE),0,1)
Controls$KQ3 <- if_else((Controls$Q14 ==TRUE),1,0)
Controls$KQ4 <- if_else((Controls$Q24 ==TRUE),0,1)
Controls$KQ5 <- if_else((Controls$Q13 ==TRUE),1,0)
Controls$KQ6 <- if_else((Controls$Q25 =="Secure"),1,0)
Controls$KQ7 <- if_else((Controls$Q26 =="True"),1,0)

Controls$KSUM <- (rowSums(Controls[,c("KQ1","KQ2","KQ3","KQ4","KQ5","KQ6","KQ7")])/7)

Controls$R1 <- Controls$Q29_1
Controls$R1 <- (Controls$R1 / 7)

prob <- c("Definitely take my winnings"=1,"Probably take my winnings"=2,"Not sure"=3,"Probably continue playing"=4,"Definitely continue playing"=5)
Controls$R2 <- prob[Controls$Q30]

likert <- c("Strongly agree"=5,"Somewhat agree"=4,"Neither agree nor disagree"=3,"Somewhat disagree"=2,"Strongly disagree"=1)
Controls$R3 <- likert[Controls$Q31]
Controls$R4 <- likert[Controls$Q32]
Controls$R5 <- likert[Controls$Q34]
Controls$R6 <- likert[Controls$Q35]
easy <- c("Extremely difficult"=1,"Somewhat difficult"=2,"Neither easy nor difficult"=3,"Somewhat easy"=4,"Extremely easy"=5)
Controls$R7 <- easy[Controls$Q36]

Controls$RSUM <- (rowSums(Controls[,c("R2","R3","R4","R5","R6","R7")])/30)
Controls$RSUMED <- (rowSums(Controls[,c("R1","RSUM")])/2)

Controls$M1 <- if_else((Controls$Q17 =="Yes"),1,0)
Controls$M2 <- if_else((Controls$Q19 =="Yes"),1,0)
Controls$M3 <- if_else((Controls$Q20 =="Yes"),1,0)

Controls$MSUM <- (rowSums(Controls[,c("M1","M2","M3")])/3)

CleanControls <- subset(Controls[,c("Q6","Q7","Q8","KSUM","RSUMED","MSUM")])

American_list <- c("American (American)","American (Caucasian)","American citizen","USA (Caucasian)","American (USA)","US Citizen","USA","US","us","American","United States",
                   "american","U.S.A.","United states","usa","U.S.")
cut_America<- paste0("\\b(", paste0(American_list, collapse="|"), ")\\b")

CleanControls$Q8 <- gsub(cut_America, "American", CleanControls$Q8)

cols <- c("Q6","Q7","Q8")
summary(CleanControls)

NewControls <- rbind(CleanControls,CleanControls,CleanControls,CleanControls,CleanControls)

```


# Fix Scores
```{r}
# Score Stacking
Scenario1 <- Scenario[,c("TimeLag1","AttributionConfidence1","DamageAssessment1","Hacktype1","Persistence1")]
Scenario2 <- Scenario[,c("TimeLag2","AttributionConfidence2","DamageAssessment2","Hacktype2","Persistence2")]
Scenario3 <- Scenario[,c("TimeLag3","AttributionConfidence3","DamageAssessment3","Hacktype3","Persistence3")]
Scenario4 <- Scenario[,c("TimeLag4","AttributionConfidence4","DamageAssessment4","Hacktype4","Persistence4")]
Scenario5 <- Scenario[,c("TimeLag5","AttributionConfidence5","DamageAssessment5",
                         "Hacktype5","Persistence5")]

# Set IDs
Scenario1$ID <- 1
Scenario2$ID <- 2
Scenario3$ID <- 3
Scenario4$ID <- 4
Scenario5$ID <- 5

dfs <- c("Scenario1", "Scenario2", "Scenario3", "Scenario4", "Scenario5")
for(df in dfs)
  assign(df, setNames(get(df),  c("Time_Lag","Attribution_Confidence","Damage_Assessment","Hack_Type","Persistence", "ID")))

Scenarios <- rbind(Scenario1,Scenario2,Scenario3,Scenario4,Scenario5)

cols <- c("Time_Lag","Attribution_Confidence","Damage_Assessment","Hack_Type","Persistence", "ID")
summary(Scenarios)

#Combine Components
stacked_data <- cbind(NewUserData,NewControls,Scenarios,NewQuestions)

levels(stacked_data$Response)[match("Escalate",levels(stacked_data$Response))] <- "Escalatory attack" # Fixed a survey answer response error affecting 1 question for 4 respondents.

# table(stacked_data$AttackDef, stacked_data$Response)
# table(stacked_data$AttackDef, stacked_data$Response,  stacked_data$Norm)
```
# Attention Check Remove Low KSUM Scores
```{r}

table(stacked_data$KSUM)
barplot(table(stacked_data$KSUM))
nrow(stacked_data[stacked_data$KSUM <= .5, ]) # 45 rows 9 students
nrow(stacked_data[stacked_data$KSUM > .5, ]) # 335 rows 67 students
nrow(stacked_data[stacked_data$KSUM > .5, ])/380 # keeps 88.16% of participants, drops 11.84
nrow(stacked_data[stacked_data$KSUM > .6, ])/380 # drops 41.6% of participants

stacked_data <- stacked_data[stacked_data$KSUM > .5, ]
table(stacked_data$KSUM)
barplot(table(stacked_data$KSUM))

summary(stacked_data)

stacked_data[14:18] <- lapply(stacked_data[14:18] , factor)
summary(stacked_data)

```

# Creating a communicativity variable
```{r}
likert2 <- c("extremely unconfident"=1,"not very confident"=2,"somewhat confident"=3,"very confident"=4,"extremely confident"=5)
stacked_data$AssessConfNo <- likert2[stacked_data$AssessConf]

stacked_data$AttackDefNo <- if_else((stacked_data$AttackDef =="Sylvania is signaling their opposition to our prior high intensity attack. They want to strategically deter attacks of this kind in the future."),2,1)

stacked_data$communicativity <- if_else(stacked_data$AttackDefNo == 2,abs(stacked_data$AssessConfNo-6),stacked_data$AssessConfNo+5)

summary(stacked_data$communicativity)
table(stacked_data$communicativity)
plot(table(stacked_data$communicativity))

table(stacked_data$AssessConf,stacked_data$Response)

# dataH4 <- lapply(dataH4, as.numeric)
# cor(stacked_data$AssessConf,stacked_data$Response)

Assessed <- as.factor(stacked_data$AssessConf)
Assessed <- as.numeric(Assessed)

Responsed <- as.factor(stacked_data$Response)
Responsed <- as.numeric(Responsed)

table(stacked_data$Q6)
# 30 No
# 305 Yes

table(stacked_data$Q7)

# 40 college graduates
# 26 professional degree
# 1 doctorate

table(stacked_data$Q8)

# 49 Americans
# 4 Indian
# 9 Unique nationality
# 4 ethnicities
# 4 blank

```

# Date Redefined to Create Cross Correlation Matrix
```{r}
unique(stacked_data$AttackDef)

stacked_data$AttackDefNeg <- if_else((stacked_data$AttackDef =="Sylvania is signaling their opposition to our prior high intensity attack. They want to strategically deter attacks of this kind in the future."),.5,-.5)

#Removing ID variable
data1 <- stacked_data %>%
  dplyr::select(communicativity,AttackDef,Response,Norm,Time_Lag:Persistence,RSUMED) %>%
  mutate(AttackDef = recode(AttackDef, "Sylvania is signaling their opposition to our prior high intensity attack. They want to strategically deter attacks of this kind in the future." = 2, "Sylvania is simply advancing its immediate interest fulfilling a tactical objective. Our prior high intensity attack was not relevant to their decision to attack us now." = 1)) %>%
  mutate(Response = recode(Response, "Deescalate" = 1, "Employ a proportional response" = 2, "Escalate" = 3, "Escalatory attack" = 3)) %>%
  mutate(Norm = recode(Norm, "Yes" = 2, "No" = 1)) %>%
  mutate(Damage_Assessment = recode(Damage_Assessment, "500 Million" = 2, "10 Million" = 1)) %>%
  mutate(Attribution_Confidence = recode(Attribution_Confidence, "90%" = 2, "60%" = 1)) %>%
  mutate(Time_Lag = recode(Time_Lag, "6 months" = 1, "1 month" = 2)) %>% 
  mutate(Hack_Type = recode(Hack_Type, "that temporarily disables a critical service" = 2, "where valuable confidential information is stolen" = 1)) %>%
  mutate(Persistence = recode(Persistence, "Sylvania has been engaging in ongoing low-impact tactical cyber operations against Freedonia." = 2, 
                              "Both nations have been engaging in ongoing low-impact tactical cyber operations against each other." = 1))

table(data1$communicativity,data1$Norm)

# Time_Lag coded so that 1 month has higher "intensity" than 6 months based on hypothesis that this would be more communicative.

dataH4 <- data1 %>%
  filter(!is.na(Norm))

dataH4 <- lapply(dataH4, as.numeric)
dataH4 <- as.data.frame(dataH4)

# cor(data1)
# data1 <- subset (data1, select = -Norm)
# data1 <- subset (data1, select = -Response)
# data1

# data1 <- data1[,-c(Norm)]

datamatrix<-cor(data1)
corrplot(datamatrix, method="number")

data1[2:9] <- lapply(data1[2:9] , factor)

table(data1$Hack_Type,data1$Time_Lag)
table(data1$Response,data1$communicativity)
summary(data1$Norm)

# class(stacked_data$Hack_Type)
# levels(stacked_data$Hack_Type)
# stacked_data$Hack_Type <- relevel(stacked_data$Hack_Type, ref = "where valuable confidential information is stolen")
  



```


# Plotting Outliers
```{r}
# Plot of data with outliers.

mod <- lm(communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence, data=stacked_data)
cooksd <- cooks.distance(mod)

sample_size <- nrow(stacked_data)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance

```
# Hypothesis 1: Attack Def rather than Communicativity
```{r}

# Hypothesis with Attack Def rather than Communicativity
logitregH1a <- glm(formula = AttackDef ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data = data1, family = "binomial")
summary(logitregH1a)

logitregH1b <- glm(formula = AttackDef ~ Damage_Assessment * Hack_Type * Attribution_Confidence + Persistence * Time_Lag, data = data1, family = "binomial")
summary(logitregH1b)
```

# Hypothesis 2: Communitivity, Divided Bimodal
```{r}
communicative <- data1 %>%
  filter(AttackDef == 1)

instrumental <- data1 %>%
  filter(AttackDef == 2)

logitreg_2I1 <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence, data = instrumental)
summary(logitreg_2I1)

logitreg_2I2 <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence, data = instrumental)
summary(logitreg_2I2)
hist(logitreg_2I2$residuals)
qqnorm(logitreg_2I2$residuals);qqline(logitreg_2I2$residuals)

logitreg_2I3 <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence + Time_Lag * Persistence, data = instrumental)
summary(logitreg_2I3)

logitreg_2I4 <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Time_Lag + Persistence, data = communicative)
summary(logitreg_2I4)

logitreg_2C1 <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence, data = communicative)
summary(logitreg_2C1)

```


# Hypothesis 3: Without interaction effects
```{r}
levels(stacked_data$Hack_Type)
levels(data1$Hack_Type)

# Hypothesis
logitregH3a <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence, data = data1)
summary(logitregH3a)

#Type a message
logitregH3b <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence, data = data1)
summary(logitregH3b)

print(xtable::xtable(logitregH1b, digits = 2, caption='Communicativity: Message Variables with Interactions'), 
      file = "logitregH3b.tex",
      caption.placement = 'top', include.colnames = TRUE)

logitregH3c <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence, data = data1)
summary(logitregH3c)

logitregH3d <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data = data1)
summary(logitregH3d)

```

# Hypothesis 4: With interaction effects
```{r}

# Hypothesis includes all possible interactions (coercive signal)
logitregH4a <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * 
                     Time_Lag, data = data1)
summary(logitregH4a)
summary(logitregH4a)$coeff[-1,4]<0.05

# Only includes interactions of the Message variables with other message variables and Context with other context variables
logitregH4b <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Damage_Assessment * Hack_Type + Damage_Assessment * 
                     Attribution_Confidence + Attribution_Confidence * Hack_Type
                   + Persistence + Time_Lag + Persistence * Time_Lag, data = data1)
summary(logitregH4b)

# Running a regression with all interaction effects between two variables (excludes interaction effects between three)
logitregH4c <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + Damage_Assessment * Hack_Type 
                   + Damage_Assessment * Attribution_Confidence + Attribution_Confidence * Hack_Type  + Persistence * Time_Lag + Damage_Assessment * Persistence
                   + Hack_Type * Persistence + Attribution_Confidence * Persistence + Damage_Assessment * Time_Lag + Hack_Type * Time_Lag + Attribution_Confidence * 
                     Time_Lag, data = data1)
summary(logitregH4c)
## high correlations are Hack_Type:Attribution_Confidence & Attribution_Confidence:Persistence & Time_Lag:Persistence

# Regression to evaluate whether interaction effects that showed high correlation in a cor table significantly alter the model
logitregH4d <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + Hack_Type*Time_Lag, data = data1) # Hack_Type*Time_Lag did not show in cor table, but is significant
summary(logitregH4d)

print(xtable::xtable(logitregH4d, digits = 2, caption='Communicativity: High Correlation Variables'), 
      file = "logitregH4d.tex",
      caption.placement = 'top', include.colnames = TRUE)

summary(logitregH4d) # no effect or significane
summary(logitregH4d)$coeff[-1,4]<0.05

```


# Hypothesis 5: Escalation
```{r}

# this subsetted data to only communicative attacks

dataH5 <- subset(data1, communicativity>5)
dataH5$Response <- factor(dataH5$Response)

# run with logit
dataH5b <- dataH5 %>%
  filter(Response != 2) %>%
  mutate(Response = recode(Response, "3" = 2, "1" = 1))

logitregH5a <- glm(Response ~ communicativity + RSUMED, data=dataH5b)
summary(logitregH5a)

logitregH5b <- glm(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + RSUMED, data=dataH5b)
summary(logitregH5b)

# run with ordinal
logitregH5c <- polr(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + RSUMED, data=dataH5)
summary(logitregH5c) # nothing is statistically significant

logitregH5d <- multinom(Response ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * Time_Lag, data=dataH5)
summary(logitregH5d) # nothing is statistically significant

# Removing the constraint of perceiving communicative attacks

logitregH5e <- multinom(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=dataH5b)
summary(logitregH5e) # nothing is statistically significant

# nothing is statistically significant
logitregH5f <- multinom(Response ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * Time_Lag, data=dataH5)
summary(logitregH5f) # nothing is statistically significant

```


# Hypothesis 6: Norm Adoption --- Future research could focus on!
```{r}
dataH6 <- dataH5 %>%
  filter(!is.na(Norm)) %>%
  mutate(Norm = recode(Norm, "No" = 0, "Yes" = 1))

nrow(dataH6) # 31 results not sufficient for statistical tests
summary(dataH6)

logitregH6a <- glm(Norm ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=dataH4)
summary(logitregH6a)

# assumptions:
# 1) identify if attack is communicative
# 2) assign confidence level to communicative assessment
# 3) choose response (escalate, proportional, deescalate) for short term response
# 4) in the long-term either abide or reject the proposed communicated norm

```

# Hypothesis 7: Effect of Supplementary Variables on Confidence
```{r}

logitregH7a <- glm(formula = AssessConfNo ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + KSUM + RSUMED + MSUM, data = stacked_data)
summary(logitregH7a)

```
 



