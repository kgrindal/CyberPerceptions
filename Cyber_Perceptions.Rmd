---
title: "Cyber_Perceptions_Survey_Proj"
author: "Karl Grindal"
date: "11/13/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# setwd<-("C:/Users/karl_000/Documents/SpiderOak Hive/GeorgiaTech")
library(dplyr)
library(qualtRics)
library(corrplot)
library(here)
library(ggplot2)
library(nnet)
library(MASS)

raw_data <- read_survey("C:/Users/karl_000/Documents/SpiderOak Hive/GeorgiaTech/Fall_2020/Cyber_Perceptions/Cyber_Perceptions_Survey_v9.csv")

# Remove Duplicative Responses
raw_data <- raw_data[!duplicated(raw_data$IPAddress), ]

anon_data <- raw_data[ , -which(names(raw_data) %in% 
                                 c("StartDate","EndDate","Status","IPAddress","RecordedDate","RecipientLastName","RecipientFirstName",
                                   "RecipientEmail","ExternalReference","LocationLatitude","LocationLongitude","role","organization"))]

write.csv(anon_data,"anon_data.csv")

```

# Clean Data for Analysis
```{r}
anon_data <- read.csv(here("anon_data.csv"), sep=",")
  
# Clean Column Names
names(anon_data) <- gsub("\\.", "", names(anon_data))

# Remove Incomplete Data
anon_data <- filter(anon_data, Progress == 100)
names(anon_data)

# Breakout Data
Userdata <- subset(anon_data[,1:which(colnames(anon_data)=="UserLanguage")])
Questions <- subset(anon_data[,which(colnames(anon_data)=="Q1"):which(colnames(anon_data)=="X5Q4")])
Controls <- subset(anon_data[,which(colnames(anon_data)=="Q11"):which(colnames(anon_data)=="Q9")])
Scenario <- subset(anon_data[,which(colnames(anon_data)=="TimeLag1" ):NCOL(anon_data)])
```

#Fix Userdata
```{r}
NewUserData <- rbind(Userdata,Userdata,Userdata,Userdata,Userdata)

```

#Fix Questions
```{r}
Questions1 <- Questions[,c("X1Q1","X1Q2","X1Q3","X1Q4")]
Questions2 <- Questions[,c("X2Q1","X2Q2","X2Q3","X2Q4")]
Questions3 <- Questions[,c("X3Q1","X3Q2","X3Q3","X3Q4")]
Questions4 <- Questions[,c("X4Q1","X4Q2","X4Q3","X4Q4")]
Questions5 <- Questions[,c("X5Q1","X5Q2","X5Q3","X5Q4")]

Questions1$ID <- 1
Questions2$ID <- 2
Questions3$ID <- 3
Questions4$ID <- 4
Questions5$ID <- 5

dfs <- c("Questions1", "Questions2", "Questions3", "Questions4", "Questions5")
for(df in dfs)
  assign(df, setNames(get(df),  c("AttackDef","AssessConf","Response","Norm", "ID")))

NewQuestions <- rbind(Questions1,Questions2,Questions3,Questions4,Questions5)

cols <- c("AttackDef","AssessConf","Response","Norm", "ID")
NewQuestions[cols] <- lapply(NewQuestions[cols], factor)  


```


# Fix Controls
```{r}
Controls$KQ1 <- if_else((Controls$Q11 ==TRUE),0,1)
Controls$KQ2 <- if_else((Controls$Q12 ==TRUE),0,1)
Controls$KQ3 <- if_else((Controls$Q14 ==TRUE),1,0)
Controls$KQ4 <- if_else((Controls$Q24 ==TRUE),0,1)
Controls$KQ5 <- if_else((Controls$Q13 ==TRUE),1,0)
Controls$KQ6 <- if_else((Controls$Q25 =="Secure"),1,0)
Controls$KQ7 <- if_else((Controls$Q26 =="True"),1,0)

Controls$KSUM <- (rowSums(Controls[,c("KQ1","KQ2","KQ3","KQ4","KQ5","KQ6","KQ7")])/7)

Controls$R1 <- Controls$Q29_1
Controls$R1 <- (Controls$R1 / 7)

prob <- c("Definitely take my winnings"=1,"Probably take my winnings"=2,"Not sure"=3,"Probably continue playing"=4,"Definitely continue playing"=5)
Controls$R2 <- prob[Controls$Q30]

likert <- c("Strongly agree"=5,"Somewhat agree"=4,"Neither agree nor disagree"=3,"Somewhat disagree"=2,"Strongly disagree"=1)
Controls$R3 <- likert[Controls$Q31]
Controls$R4 <- likert[Controls$Q32]
Controls$R5 <- likert[Controls$Q34]
Controls$R6 <- likert[Controls$Q35]
easy <- c("Extremely difficult"=1,"Somewhat difficult"=2,"Neither easy nor difficult"=3,"Somewhat easy"=4,"Extremely easy"=5)
Controls$R7 <- easy[Controls$Q36]

Controls$RSUM <- (rowSums(Controls[,c("R2","R3","R4","R5","R6","R7")])/30)
Controls$RSUMED <- (rowSums(Controls[,c("R1","RSUM")])/2)

Controls$M1 <- if_else((Controls$Q17 =="Yes"),1,0)
Controls$M2 <- if_else((Controls$Q19 =="Yes"),1,0)
Controls$M3 <- if_else((Controls$Q20 =="Yes"),1,0)

Controls$MSUM <- (rowSums(Controls[,c("M1","M2","M3")])/3)

CleanControls <- subset(Controls[,c("Q6","Q7","Q8","KSUM","RSUMED","MSUM")])

American_list <- c("American (American)","American (Caucasian)","American citizen","USA (Caucasian)","American (USA)","US Citizen","USA","US","us","American","United States",
                   "american","U.S.A.","United states","usa","U.S.")
cut_America<- paste0("\\b(", paste0(American_list, collapse="|"), ")\\b")

CleanControls$Q8 <- gsub(cut_America, "American", CleanControls$Q8)

cols <- c("Q6","Q7","Q8")
CleanControls[cols] <- lapply(CleanControls[cols], factor)  

summary(CleanControls)

NewControls <- rbind(CleanControls,CleanControls,CleanControls,CleanControls,CleanControls)



```


# Fix Scores
```{r}
# Score Stacking
Scenario1 <- Scenario[,c("TimeLag1","AttributionConfidence1","DamageAssessment1","Hacktype1","Persistence1")]
Scenario2 <- Scenario[,c("TimeLag2","AttributionConfidence2","DamageAssessment2","Hacktype2","Persistence2")]
Scenario3 <- Scenario[,c("TimeLag3","AttributionConfidence3","DamageAssessment3","Hacktype3","Persistence3")]
Scenario4 <- Scenario[,c("TimeLag4","AttributionConfidence4","DamageAssessment4","Hacktype4","Persistence4")]
Scenario5 <- Scenario[,c("TimeLag5","AttributionConfidence5","DamageAssessment5",
                         "Hacktype5","Persistence5")]


# Set IDs
Scenario1$ID <- 1
Scenario2$ID <- 2
Scenario3$ID <- 3
Scenario4$ID <- 4
Scenario5$ID <- 5

dfs <- c("Scenario1", "Scenario2", "Scenario3", "Scenario4", "Scenario5")
for(df in dfs)
  assign(df, setNames(get(df),  c("Time_Lag","Attribution_Confidence","Damage_Assessment","Hack_Type","Persistence", "ID")))

Scenarios <- rbind(Scenario1,Scenario2,Scenario3,Scenario4,Scenario5)

cols <- c("Time_Lag","Attribution_Confidence","Damage_Assessment","Hack_Type","Persistence", "ID")
Scenarios[cols] <- lapply(Scenarios[cols], factor)  

summary(Scenarios)

#Combine Components
stacked_data <- cbind(NewUserData,NewControls,Scenarios,NewQuestions)

levels(stacked_data$Response)[match("Escalate",levels(stacked_data$Response))] <- "Escalatory attack" # Fixed a survey answer response error affecting 1 question for 4 respondents.

# table(stacked_data$AttackDef, stacked_data$Response)
# table(stacked_data$AttackDef, stacked_data$Response,  stacked_data$Norm)

table(stacked_data$KSUM)
barplot(table(stacked_data$KSUM))
nrow(stacked_data[stacked_data$KSUM > .5, ])/385 # drops 11.7% of participants
nrow(stacked_data[stacked_data$KSUM > .6, ])/385 # drops 41.6% of participants

# stacked_data <- stacked_data[stacked_data$KSUM > .5714, ]
nrow(stacked_data)

```



# Creating a communicativity variable
```{r}
summary(stacked_data$communicativity)
table(stacked_data$communicativity)

stacked_data$AssessConf <- as.character(stacked_data$AssessConf)

stacked_data$AttackDefNeg <- if_else((stacked_data$AttackDef =="Sylvania is signaling their opposition to our prior high intensity attack. They want to strategically deter attacks of this kind in the future."),.5,-.5)

class(stacked_data$'AttackDefNo')
class(stacked_data$'ResponseId')


likert2 <- c("extremely unconfident"=1,"not very confident"=2,"somewhat confident"=3,"very confident"=4,"extremely confident"=5)
stacked_data$AssessConfNo <- likert2[stacked_data$AssessConf]

stacked_data$AttackDefNo <- if_else((stacked_data$AttackDef =="Sylvania is signaling their opposition to our prior high intensity attack. They want to strategically deter attacks of this kind in the future."),2,1)
stacked_data$AttackDefNo <- as.numeric(stacked_data$AttackDefNo)

stacked_data$communicativity <- if_else(stacked_data$AttackDefNo == 2,abs(stacked_data$AssessConfNo-6),stacked_data$AssessConfNo+5)

stacked_data$communicativity <- as.numeric(stacked_data$communicativity)


```

# Date Reducation to Creat Cross Correlation Matrix
```{r}
#Removing ID variable

data1 <- subset(stacked_data, select = c("communicativity","Response","Damage_Assessment","Hack_Type","Attribution_Confidence","Persistence","Time_Lag"))

data1$Damage_Assessment <- as.numeric(data1$Damage_Assessment)
data1$Hack_Type <- as.numeric(data1$Hack_Type)
data1$Attribution_Confidence <- as.numeric(data1$Attribution_Confidence)
data1$Persistence <- as.numeric(data1$Persistence)
data1$Time_Lag <- as.numeric(data1$Time_Lag)
data1$Response <- as.numeric(data1$Response)

datamatrix<-cor(data1)
corrplot(datamatrix, method="number")

```


# Plotting Outliers
```{r}

# Plot of data with outliers.

plot1 <- ggplot(data = data1, aes(x = Attribution_Confidence, y = communicativity)) +
        geom_point() + 
        geom_smooth(method = lm) +
        xlim(0, 3) + ylim(0, 10) + 
        ggtitle("No Outliers")
plot2 <- ggplot(data = data1, aes(x = Attribution_Confidence, y = communicativity)) +
        geom_point() + 
        geom_smooth(method = lm) +
        xlim(0, 3) + ylim(0, 10) + 
        ggtitle("With Outliers")

gridExtra::grid.arrange(plot1, plot2, ncol=2)


mod <- lm(communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence, data=stacked_data)
cooksd <- cooks.distance(mod)

sample_size <- nrow(stacked_data)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance

```

# Hypothesis 1: Without interaction effects
```{r}

# Hypothesis 

logitregH1 <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence, data = stacked_data)
summary(logitregH1)

#Type a message

logitregH1b <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence, data = stacked_data)
summary(logitregH1b)

logitregH1c <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence, data = stacked_data)
summary(logitregH1c)

logitregH1d <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + KSUM + RSUMED + MSUM, data = stacked_data)
summary(logitregH1d)

```

# Hypothesis 2: With interaction effects
```{r}

# Hypothesis includes all possible interactions (coercive signal)
logitregH2a <- glm(formula = communicativity ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * 
                     Time_Lag, data = data1)
summary(logitregH2a)
summary(logitregH2a)$coeff[-1,4]<0.05

# Only includes interactions of the Message variables with other message variables and Context with other context variables
logitregH2b <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Damage_Assessment * Hack_Type + Damage_Assessment * 
                     Attribution_Confidence + Attribution_Confidence * Hack_Type
                   + Persistence + Time_Lag + Persistence * Time_Lag, data = data1)
summary(logitregH2b)

# Running a regression with all interaction effects betweent two variables (excludes interaction effects between three)
logitregH2c <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + Damage_Assessment * Hack_Type 
                   + Damage_Assessment * Attribution_Confidence + Attribution_Confidence * Hack_Type  + Persistence * Time_Lag + Damage_Assessment * Persistence
                   + Hack_Type * Persistence + Attribution_Confidence * Persistence + Damage_Assessment * Time_Lag + Hack_Type * Time_Lag + Attribution_Confidence * 
                     Time_Lag, data = data1)
summary(logitregH2c)
## high correlations are Hack_Type:Attribution_Confidence & Attribution_Confidence:Persistence & Time_Lag:Persistence

# Regression to evaluate whether interaction effects that showed high correlation in a cor table significantly alter the model
logitregH2d <- glm(formula = communicativity ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + Hack_Type*Time_Lag, data = data1) # Hack_Type*Time_Lag did not show in cor table, but is significant

summary(logitregH2d) # no effect or significane
summary(logitregH2d)$coeff[-1,4]<0.05

```


# Hypothesis 3 on Escalation
```{r}

# this subsetted data to only communicative attacks

dataH3 <- subset(data1, communicativity>5)
dataH3$Response <- factor(dataH3$Response)

# run with multinom
logitregH3a <- multinom(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=dataH3)
summary(logitregH3a) # nothing is statistically significant

# run with ordinal
logitregH3a2 <- polr(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=dataH3)
summary(logitregH3a2) # nothing is statistically significant

logitregH3b <- multinom(Response ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * Time_Lag, data=dataH3)
summary(logitregH3b) # nothing is statistically significant

# Removing the constraint of perceiving communicative attacks

logitregH3c <- multinom(Response ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=data1)
summary(logitregH3c) # nothing is statistically significant

# nothing is statistically significant
logitregH3d <- multinom(Response ~ Damage_Assessment * Hack_Type * Attribution_Confidence * Persistence * Time_Lag, data=data1)
summary(logitregH3d) # nothing is statistically significant


```


# Hypothesis 4 on Norm Adoption --- Future research could focus on!
```{r}

dataH4 <- na.omit(dataH3)

nrow(dataH4) # 31 results not sufficient for statistical tests
summary(dataH4)

# logitregH4a <- glm(Norm ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag, data=dataH4)
# summary(logitregH4a)

# assumptions:
# 1) identify if attack is communicative
# 2) assign confidence level to communicative assessment
# 3) choose response (escalate, proportional, deescalate) for short term response
# 4) in the long-term either abide or reject the proposed communicated norm

```

# Hypothesis 5 on Effect of Supplementary Variables on Confidence
```{r}

logitregH5a <- glm(formula = AssessConfNo ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + KSUM + RSUMED + MSUM, data = stacked_data)
summary(logitregH5a)

summary(stacked_data$KSUM)

```
# Hypothesis 6 Employing a logit on instrumentality vs communicativity

```{r}

stacked_data$AttackDefNo <- (stacked_data$AttackDefNo - 1)
logitregH6a <- glm(formula = AttackDefNo ~ Damage_Assessment + Hack_Type + Attribution_Confidence + Persistence + Time_Lag + KSUM + RSUMED + MSUM, family = "binomial", 
                   data = stacked_data) 
summary(logitregH6a)
```






